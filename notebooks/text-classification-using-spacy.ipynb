{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0715c83044a6f612c86e5315b22c16c2d02bf173"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "#### About Dataset:\n",
    "We will be using rich dataset of amazon fine food reviews.\n",
    "\n",
    "####  What are we trying to achieve??\n",
    "We are going to tackle an interesting natural language processing problem i.e sentiment or text classification.\n",
    "We will explore texual data using amazing spaCy library and build a text classification model.\n",
    "\n",
    "### Here is breakdown of concepts I will try to explain.\n",
    "We will extract linguistic features like \n",
    "1. tokenization,\n",
    "1. part-of-speech tagging, \n",
    "1. dependency parsing, \n",
    "1. lemmatization , \n",
    "1. named entities recognition,\n",
    "1. Sentence Boundary Detection\t\n",
    "for building language models later.\n",
    "\n",
    "Visualizing Data\n",
    "1. explacy - explaining how parsing is done\n",
    "1. displaCy - visualizing named entities\n",
    "\n",
    "Word vectors and similarity\n",
    "1. sense2vec - using contextual information for building word embeddings\n",
    "\n",
    "Text classification model\n",
    "1. SpaCy TextCategorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2551b57f3a848ae2f6e2951952922b8de35d370a"
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "9919e0bb871106a57320152b6159a79a361a92d3"
   },
   "outputs": [],
   "source": [
    "import explacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../tests/data/Reviews.zip\n",
      "replace tests/data/Reviews.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: tests/data/Reviews.csv  ^C\n"
     ]
    }
   ],
   "source": [
    "! set -e\n",
    "!unzip ../tests/data/Reviews.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "78b0ec0fe0ac689ae1f4b56a9a87aaa85a4ad28b"
   },
   "source": [
    "Let's read in food reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "3249bd33a5451044bff7a4513dfe4baaef6c1efb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/amazon-fine-food-reviews/Reviews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m food_reviews_df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../input/amazon-fine-food-reviews/Reviews.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m food_reviews_df\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/mambaforge/envs/text_classification/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/text_classification/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/mambaforge/envs/text_classification/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/text_classification/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/mambaforge/envs/text_classification/lib/python3.9/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/amazon-fine-food-reviews/Reviews.csv'"
     ]
    }
   ],
   "source": [
    "food_reviews_df=pd.read_csv('../tests/data/Reviews.csv')\n",
    "food_reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5eab874d485e4b353f28d01d1450e5f35a3103f6"
   },
   "outputs": [],
   "source": [
    "food_reviews_df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cfa3619255af6836b75b9afa22856ba92021733e"
   },
   "source": [
    "Text column contains review given by customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f10a32c6f40f091c2ed376d2354f849ef5deaa22"
   },
   "source": [
    "Let's focus on texual data and ratings for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2beca8fa08a7e5f57a185e81843064780a270938"
   },
   "outputs": [],
   "source": [
    "food_reviews_df = food_reviews_df[['Text','Score']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "934b2d98b61f7acdadc3c95db2a1c966bd8d1188"
   },
   "outputs": [],
   "source": [
    "ax=food_reviews_df.Score.value_counts().plot(kind='bar')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"score.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8a02ea9da92ee2fafbeb81e7c1d815d1038dd20"
   },
   "source": [
    "We have five-star rating system.\n",
    "It looks like we have more reviews with ratings 5, this can lead to unbalanced classes. We will treat rating 4 and 5 as positive and rest as negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "5c35968c0535457a171dc620298dbf4863d6f269"
   },
   "outputs": [],
   "source": [
    "food_reviews_df.Score[food_reviews_df.Score<=3]=0\n",
    "food_reviews_df.Score[food_reviews_df.Score>=4]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a29d89ea63f0cc365fd57f58834f6f678940059"
   },
   "outputs": [],
   "source": [
    "ax=food_reviews_df.Score.value_counts().plot(kind='bar')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"score_boolean.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21249c348db38d135c4d6d9a560fa829d009c430"
   },
   "outputs": [],
   "source": [
    "food_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3015c49982aac3b69f4e286760dc66c8656341cb"
   },
   "source": [
    "Since we have huge data, since it might be difficult to train in kernel, I will reduce data size of 100K rows.\n",
    "To balance classes, i have selected equal samples from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c90e5fb27a1a10707853d470c4354f321b2f479"
   },
   "outputs": [],
   "source": [
    "train_pos_df=food_reviews_df[food_reviews_df.Score==1][:50000]\n",
    "train_neg_df=food_reviews_df[food_reviews_df.Score==0][:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ebb304fcb88339861e2dfe4c865b91e6870d2cea"
   },
   "outputs": [],
   "source": [
    "train_df=train_pos_df.append(train_neg_df)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "b439885ac9288394481948ab400fce8a94593153"
   },
   "outputs": [],
   "source": [
    "val_pos_df=food_reviews_df[food_reviews_df.Score==1][50000:60000]\n",
    "val_neg_df=food_reviews_df[food_reviews_df.Score==0][50000:60000]\n",
    "val_df=val_pos_df.append(val_neg_df)\n",
    "val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b8aca580867d3767e3c3b24f55cc1db443f365b",
    "heading_collapsed": true
   },
   "source": [
    "### Linguistic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b28ff4da665c5bde9a7a4b85d937993071f35c2",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Tokenization\n",
    "First step in any nlp pipeline is tokenizing text i.e breaking down paragraphs into sentenses and then sentenses into words, punctuations and so on.\n",
    "\n",
    "we will load english language model to tokenize our english text.\n",
    "\n",
    "Every language is different and have different rules. Spacy offers 8 different language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c1fef3ada345d4ea8d4b8ceac51d317151a231f",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spacy_tok = spacy.load('en_core_web_sm')\n",
    "sample_review=food_reviews_df.Text[54]\n",
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d34364d38de3125bc8f9473fa8a3bac2289e4fa5",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parsed_review = spacy_tok(sample_review)\n",
    "parsed_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21ef54c7df2f3ff4c58184d6abc2a9d4dd2e0811"
   },
   "source": [
    "There is not much difference between parsed review and original one. But we will see ahead what has actually happened.\n",
    "We can see how parsing has been done visually through **explacy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "686b7fb23a5d33abc9080dc110df6ee113eff650",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/tylerneylon/explacy/master/explacy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8383d83c813120f39e6960530fbb353870391162",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "explacy.print_parse_info(spacy_tok, 'The salad was surprisingly tasty.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c0325c2c9e89878e668ea843c0694d13c04fe18"
   },
   "outputs": [],
   "source": [
    "explacy.print_parse_info(spacy_tok,food_reviews_df.Text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2797fa3eadbf5b553c80a4a269c69e9870a0cc25",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Part-of-speech tagging\n",
    "After tokenization we can parse and tag variety of parts of speech to paragraph text. SpaCy uses statistical models in background to predict which tag will go for each word(s) based on the context.\n",
    "\n",
    "##### Lemmatization\n",
    "It is the process of extracting uninflected/base form of the word.\n",
    "Lemma can be like\n",
    "For eg. \n",
    "\n",
    "Adjectives: best, better → good\n",
    "Adverbs: worse, worst → badly\n",
    "Nouns: ducks, children → duck, child\n",
    "Verbs: standing,stood → stand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f3afec00062f8b3395bf2369dae40a6eaf5400b5",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenized_text = pd.DataFrame()\n",
    "\n",
    "for i, token in enumerate(parsed_review):\n",
    "    tokenized_text.loc[i, 'text'] = token.text\n",
    "    tokenized_text.loc[i, 'lemma'] = token.lemma_,\n",
    "    tokenized_text.loc[i, 'pos'] = token.pos_\n",
    "    tokenized_text.loc[i, 'tag'] = token.tag_\n",
    "    tokenized_text.loc[i, 'dep'] = token.dep_\n",
    "    tokenized_text.loc[i, 'shape'] = token.shape_\n",
    "    tokenized_text.loc[i, 'is_alpha'] = token.is_alpha\n",
    "    tokenized_text.loc[i, 'is_stop'] = token.is_stop\n",
    "    tokenized_text.loc[i, 'is_punctuation'] = token.is_punct\n",
    "\n",
    "tokenized_text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8bb42b294ecaf912cc5cc3749c7955dcafa02b3",
    "hidden": true
   },
   "source": [
    "#### Named Entity Recognition (NER)\n",
    "Named entity is real world object like Person, Organization etc\n",
    "\n",
    "Spacy figures out below entities automatically:\n",
    "\n",
    "|Type\t|Description|\n",
    "|------|------|\n",
    "|PERSON|\tPeople, including fictional.\n",
    "|NORP|\tNationalities or religious or political groups.|\n",
    "|FAC|\tBuildings, airports, highways, bridges, etc.|\n",
    "|ORG|\tCompanies, agencies, institutions, etc.|\n",
    "|GPE|\tCountries, cities, states.|\n",
    "|LOC|\tNon-GPE locations, mountain ranges, bodies of water.|\n",
    "|PRODUCT|\tObjects, vehicles, foods, etc. (Not services.)|\n",
    "|EVENT|\tNamed hurricanes, battles, wars, sports events, etc.|\n",
    "|WORK_OF_ART|\tTitles of books, songs, etc.|\n",
    "|LAW|\tNamed documents made into laws.|\n",
    "|LANGUAGE|\tAny named language.|\n",
    "|DATE|\tAbsolute or relative dates or periods.|\n",
    "|TIME|\tTimes smaller than a day.|\n",
    "|PERCENT|\tPercentage, including \"%\".|\n",
    "|MONEY|\tMonetary values, including unit.|\n",
    "|QUANTITY|\tMeasurements, as of weight or distance.|\n",
    "|ORDINAL|\t\"first\", \"second\", etc.|\n",
    "|CARDINAL|\tNumerals that do not fall under another type|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a6f77fec8bd915e7499375822c253fc5742edf7",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spacy.displacy.render(parsed_review, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5de88086fb1412a1f38cc1af135d1cb35dd8a6d"
   },
   "outputs": [],
   "source": [
    "spacy.explain('GPE') # to explain POS tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14d10f9d14b578add72f3894d419db4083e71f76",
    "hidden": true
   },
   "source": [
    "#### Dependency parsing\n",
    "Syntactic Parsing or Dependency Parsing is process of identifyig sentenses and assigning a syntactic structure to it.\n",
    "As in Subject combined with object makes a sentence.\n",
    "Spacy provides parse tree which can be used to generate this structure.\n",
    "\n",
    "##### Sentense Boundry Detection\n",
    "Figuring out where sentense starts and ends is very imporatnt part of nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4622f182416f31136c456c7da33d3d35c48f7454",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentence_spans = list(parsed_review.sents)\n",
    "sentence_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e46015628663109cd690c3a10b43c2342ee2fd4b",
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "displacy.render(parsed_review, style='dep', jupyter=True,options={'distance': 140})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a06fe267e43251696b85ab0d9360babac2ae452",
    "hidden": true
   },
   "source": [
    "Kindly scroll down if you can't see the output above.\n",
    "You can even customize dependency parser's output as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "370cb18088dcb6d7921b0cc0a0a99ef1791f465e",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "options = {'compact': True, 'bg': 'violet','distance': 140,\n",
    "           'color': 'white', 'font': 'Trebuchet MS'}\n",
    "displacy.render(parsed_review, jupyter=True, style='dep', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8bbf982767f2d6c7a8023014ef0e4846c3b0853"
   },
   "outputs": [],
   "source": [
    "spacy.explain(\"ADJ\") ,spacy.explain(\"det\") ,spacy.explain(\"ADP\") ,spacy.explain(\"prep\")  # to understand tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f44109e7d3bba0f4233db5e362ea230a14d62ca3",
    "hidden": true
   },
   "source": [
    "#### Processing Noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8521bb6e4b35a635ddb07dd842064769af13b762",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "noun_chunks_df = pd.DataFrame()\n",
    "\n",
    "for i, chunk in enumerate(parsed_review.noun_chunks):\n",
    "    noun_chunks_df.loc[i, 'text'] = chunk.text\n",
    "    noun_chunks_df.loc[i, 'root'] = chunk.root,\n",
    "    noun_chunks_df.loc[i, 'root.text'] = chunk.root.text,\n",
    "    noun_chunks_df.loc[i, 'root.dep_'] = chunk.root.dep_\n",
    "    noun_chunks_df.loc[i, 'root.head.text'] = chunk.root.head.text\n",
    "\n",
    "noun_chunks_df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "36bdab6cdaa456cd06ea7ddc98a1515ed70e54ab"
   },
   "source": [
    "### Visualizing using Scattertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "d08938394af752f96b7ea23baf76d411ec1eb518"
   },
   "outputs": [],
   "source": [
    "!pip install scattertext\n",
    "import scattertext as st\n",
    "nlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8513457e5fefc2752b49652e1c3305c67fc5f90"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])\n",
    "train_df['parsed'] = train_df.Text[49500:50500].apply(nlp)\n",
    "corpus = st.CorpusFromParsedDocuments(train_df[49500:50500],\n",
    "                             category_col='Score',\n",
    "                             parsed_col='parsed').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "2b45bbf1fe689c35173a29b5203f1099e9e2abec"
   },
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(corpus,\n",
    "          category=1,\n",
    "          category_name='Positive',\n",
    "          not_category_name='Negative',\n",
    "          width_in_pixels=700,\n",
    "          minimum_term_frequency=15,\n",
    "          term_significance = st.LogOddsRatioUninformativeDirichletPrior(),\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "6adc5e405d629097c8f56d476b8975108d055c59"
   },
   "outputs": [],
   "source": [
    "# uncomment this cell to load the interactive scattertext visualisation\n",
    "filename = \"positive-vs-negative.html\"\n",
    "open(filename, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=filename, width = 900, height=900)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05f9d0cd5eacd1a1c56da174203cb96e88fb00a7"
   },
   "source": [
    "### Word vectors and similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a7df3d1f449bdb569766202ffe20890949409fc0"
   },
   "source": [
    "Ok let's do some modelling and focus on scoring our food!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a88a94ba02c9b4e7f57f5f2159d402c042e88e6e",
    "heading_collapsed": true
   },
   "source": [
    "### Sence2vec\n",
    "\n",
    "The idea is get something better than word2vec model.\n",
    "\n",
    "The idea behind sense2vec is super simple. If the problem is that duck as in waterfowl and duck as in crouch are different concepts, the straight-forward solution is to just have two entries, duckN and duckV.  Trask et al (2015) published a nice set of experiments showing that the idea worked well.\n",
    "\n",
    "It assight parts of speech tags like verb, noun , adjective to words, which will in turn be used to make sence of context.\n",
    "1. Please book [VERB] my ticket.\n",
    "2. Read the book [NOUN].\n",
    "\n",
    "Read more [here](https://explosion.ai/blog/sense2vec-with-spacy) and [here](https://github.com/explosion/sense2vec)\n",
    "\n",
    "Reddit talks about food a lot so we can get nice similarity vectors for food items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "39c873570d123f3e96b71ad3506ac3b7e6a7d6ee",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!pip install sense2vec==1.0.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a18f79503c8bc8389baab0a41d550a782a9cdced",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sense2vec\n",
    "from sense2vec import Sense2VecComponent\n",
    "\n",
    "s2v = Sense2VecComponent('../input/reddit-vectors-for-sense2vec-spacy/reddit_vectors-1.1.0/reddit_vectors-1.1.0/')\n",
    "spacy_tok.add_pipe(s2v)\n",
    "doc = spacy_tok(u\"dessert.\")\n",
    "freq = doc[0]._.s2v_freq\n",
    "vector = doc[0]._.s2v_vec\n",
    "most_similar = doc[0]._.s2v_most_similar(5)\n",
    "most_similar,freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ab1d9e89fd7cae0cfc95f7b21ad90507a7e02e2",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc = spacy_tok(u\"burger\")\n",
    "most_similar = doc[0]._.s2v_most_similar(4)\n",
    "most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7fb72a57656ef9e21930534e5bb9dbc956c77ad0",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc = spacy_tok(u\"peanut butter\")\n",
    "most_similar = doc[0]._.s2v_most_similar(4)\n",
    "most_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f114333faf40dbff7dcf9d4939de7f0e554141ce"
   },
   "source": [
    "Similarity between entities can be kind of fun.\n",
    "\n",
    "\n",
    "The following attributes are available via the ._ property – for example token._.in_s2v:\n",
    "\n",
    "Name\t|Attribute Type|\tType|\tDescription|\n",
    "--------|---------------|-------------|---------------|\n",
    "in_s2v\t|property|\tbool|\tWhether a key exists in the vector map.\n",
    "s2v_freq|\tproperty|\tint|\tThe frequency of the given key.\n",
    "s2v_vec|\tproperty|\tndarray[float32]|\tThe vector of the given key.\n",
    "s2v_most_similar|\tmethod|\tlist|\tGet the n most similar terms. Returns a list of ((word, sense), score) tuples.\n",
    "\n",
    "\n",
    "\n",
    "## SpaCy Text Categorizer\n",
    "\n",
    "We will train a multi-label convolutional neural network text classifier on our food reviews, using spaCy's new TextCategorizer  component.\n",
    "\n",
    "SpaCy provides classification model with multiple, non-mutually exclusive labels. You can change the model architecture rather easily, but by default, the TextCategorizer class uses a convolutional neural network to assign position-sensitive vectors to each word in the document. The TextCategorizer uses its own CNN model, to avoid sharing weights with the other pipeline components. The document tensor is then summarized by concatenating max and mean pooling, and a multilayer perceptron is used to predict an output vector of length nr_class, before a logistic activation is applied elementwise. The value of each output neuron is the probability that some class is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ffce959d65f1761f9b31545ad82c35f509a4f54"
   },
   "source": [
    "#### Prepare data\n",
    "Let's prepare the data as SpaCy would like it.\n",
    "It accepts list of tuples of text and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62289b6556a5b63e4c85e825f5fe5f2f0ff0e105"
   },
   "outputs": [],
   "source": [
    "train_df['tuples'] = train_df.apply(\n",
    "    lambda row: (row['Text'],row['Score']), axis=1)\n",
    "train = train_df['tuples'].tolist()\n",
    "train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40af723e2e6c7dba39f2a4efe6ea28cae69087bf"
   },
   "outputs": [],
   "source": [
    "train[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dbcea3ccdb03ee795d8d0c9b65d2ddcd055b3b3"
   },
   "outputs": [],
   "source": [
    "#functions from spacy documentation\n",
    "def load_data(limit=0, split=0.8):\n",
    "    train_data = train\n",
    "    np.random.shuffle(train_data)\n",
    "    train_data = train_data[-limit:]\n",
    "    texts, labels = zip(*train_data)\n",
    "    cats = [{'POSITIVE': bool(y)} for y in labels]\n",
    "    split = int(len(train_data) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n",
    "\n",
    "#(\"Number of texts to train from\",\"t\" , int)\n",
    "n_texts=30000\n",
    "#You can increase texts count if you have more computational power.\n",
    "\n",
    "#(\"Number of training iterations\", \"n\", int))\n",
    "n_iter=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a7ba3ef169e93644c6d324c030e5b0065aa9e13"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')  # create english Language class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e913351d8c6340008621eec634f19a448a07f478"
   },
   "outputs": [],
   "source": [
    "# add the text classifier to the pipeline if it doesn't exist\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe('textcat')\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "# otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "# add label to text classifier\n",
    "textcat.add_label('POSITIVE')\n",
    "\n",
    "# load the dataset\n",
    "print(\"Loading food reviews data...\")\n",
    "(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n",
    "print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "      .format(n_texts, len(train_texts), len(dev_texts)))\n",
    "train_data = list(zip(train_texts,\n",
    "                      [{'cats': cats} for cats in train_cats]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf90757df96b0020e96f524ae2a799ffeb92c2d8"
   },
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd483f12ee2663acb586f9c9b7dc69dc3784791b"
   },
   "outputs": [],
   "source": [
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "    print(\"Training the model...\")\n",
    "    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                       losses=losses)\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            # evaluate on the dev data split off in load_data()\n",
    "            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n",
    "              .format(losses['textcat'], scores['textcat_p'],\n",
    "                      scores['textcat_r'], scores['textcat_f']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa8aed6aebeb36b24bed6bb71e78efbfaf1b0bbd"
   },
   "outputs": [],
   "source": [
    "# test the trained model\n",
    "test_text1 = 'This tea is fun to watch as the flower expands in the water. Very smooth taste and can be used again and again in the same day. If you love tea, you gotta try these \"flowering teas\"'\n",
    "test_text2=\"I bought this product at a local store, not from this seller.  I usually use Wellness canned food, but thought my cat was bored and wanted something new.  So I picked this up, knowing that Evo is a really good brand (like Wellness).<br /><br />It is one of the most disgusting smelling cat foods I've ever had the displeasure of using.  I was gagging while trying to put it into the bowl.  My cat took one taste and walked away, and chose to eat nothing until I replaced it 12 hours later with some dry food.  I would try another flavor of their food - since I know it's high quality - but I wouldn't buy the duck flavor again.\"\n",
    "doc = nlp(test_text1)\n",
    "test_text1, doc.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1634b107850fd2259fc4a66e6ea1704e1d12cf12"
   },
   "source": [
    "Positive review is indeed close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "11d75e3ebbed865e81f3c77a6b7e50367218d60c"
   },
   "outputs": [],
   "source": [
    "doc2 = nlp(test_text2)\n",
    "test_text2, doc2.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6bcfa478277ddf06b5bfc3f23c9fa454c0fd979e"
   },
   "source": [
    "Negative review is close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "811972b7bffd711bcd64954ab5d7d94b607bfea3"
   },
   "outputs": [],
   "source": [
    "output_dir=%pwd\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a88593022d2daef645cff7bc4f849dabc7d31795"
   },
   "outputs": [],
   "source": [
    "# test the saved model\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "doc2 = nlp2(test_text2)\n",
    "print(test_text2, doc2.cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5da42c208abfe16c37fbb177b1f641d584b91068"
   },
   "source": [
    "Model looks preety good. We can definitely improve it further by feeding more data and data augmentations.\n",
    "Thanks for reading. Hope you learnt something new :)  #TODO Data Augmentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
